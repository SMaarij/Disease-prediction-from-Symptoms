{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL TRAINING(Random Forest Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest model\n",
    "from sklearn.preprocessing import LabelEncoder  # Label encoding for target variable\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix  # Evaluation metrics\n",
    "from sklearn.model_selection import train_test_split  # To split the dataset\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import seaborn as sns  # Data visualization library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = pd.read_csv('Dataset\\\\training_data.csv')  # Read the training dataset\n",
    "df.head()  # Display the first few rows of the dataset to understand its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "encoder = LabelEncoder()  # Initialize the label encoder\n",
    "df['prognosis'] = encoder.fit_transform(df.prognosis)  # Encode the 'prognosis' column to convert categorical labels to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary column\n",
    "df = df.drop(['Unnamed: 133'], axis=1)  # Drop the 'Unnamed: 133' column, which is not needed for the analysis\n",
    "df.head()  # Display the first few rows of the dataset after dropping the column to verify changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlations\n",
    "df.corr()  # Calculate and display the correlation matrix to understand relationships between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()  # Check for missing values in each column to ensure data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = df.iloc[:, :-1]  # Select all columns except the last one as features\n",
    "y = df.iloc[:, -1]  # Select the last column as the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store accuracies\n",
    "max_depth_values = range(1, 21)\n",
    "training_accuracy_values = []\n",
    "validation_accuracy_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForestClassifier with different max_depth values\n",
    "for max_depth in max_depth_values:\n",
    "    rfc = RandomForestClassifier(max_depth=max_depth, random_state=0)  # Initialize the Random Forest Classifier with max_depth\n",
    "    rfc.fit(X_train, y_train)  # Fit the model to the training data to train the classifier\n",
    "    train_acc = accuracy_score(y_train, rfc.predict(X_train))  # Calculate training accuracy\n",
    "    val_acc = accuracy_score(y_val, rfc.predict(X_val))  # Calculate validation accuracy\n",
    "    training_accuracy_values.append(train_acc)\n",
    "    validation_accuracy_values.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between max_depth and accuracy\n",
    "plt.plot(max_depth_values, training_accuracy_values, label=\"Train Accuracy\")\n",
    "plt.plot(max_depth_values, validation_accuracy_values, label=\"Validation Accuracy\")\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Relationship between max_depth and accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing data\n",
    "test = pd.read_csv('Dataset\\\\test_data.csv')  # Read the testing dataset\n",
    "test.head()  # Display the first few rows of the testing dataset to understand its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable in test data\n",
    "test['prognosis'] = encoder.transform(test.prognosis)  # Encode the 'prognosis' column in the test data to match training data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test features and target\n",
    "testx = test.iloc[:, :-1]  # Select all columns except the last one as features in the test data\n",
    "testy = test.iloc[:, -1]  # Select the last column as the target variable in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the trained model with optimal max_depth\n",
    "optimal_rfc = RandomForestClassifier(max_depth=10, random_state=0)  # Using max_depth=10\n",
    "optimal_rfc.fit(X_train, y_train)\n",
    "y_pred = optimal_rfc.predict(testx)  # Predict the target variable for the test data using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(f\"Accuracy on train data by Random Forest Classifier: {accuracy_score(y_train, optimal_rfc.predict(X_train)) * 100:.2f}%\")  # Print the accuracy on the training data to evaluate model performance\n",
    "print(f\"Accuracy on test data by Random Forest Classifier: {accuracy_score(testy, y_pred) * 100:.2f}%\")  # Print the accuracy on the test data to evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cf_matrix = confusion_matrix(testy, y_pred)  # Compute the confusion matrix to understand the performance of the classifier\n",
    "plt.figure(figsize=(12, 8))  # Set the figure size for the plot\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='d')  # Plot the confusion matrix with annotations to visualize the results\n",
    "plt.title(\"Confusion Matrix for Random Forest Classifier on Test Data\")  # Set the title of the plot for clarity\n",
    "plt.show()  # Display the plot to visualize the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier Without Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Function to load CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for row in csv_reader:\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Function to convert string column to float (if applicable)\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Function to convert string column to integer (if applicable)\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = {value: index for index, value in enumerate(unique)}\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Function to split dataset into training and testing sets\n",
    "def train_test_split(dataset, split_ratio):\n",
    "    train_size = int(len(dataset) * split_ratio)\n",
    "    train_set = []\n",
    "    test_set = list(dataset)\n",
    "    while len(train_set) < train_size:\n",
    "        index = random.randrange(len(test_set))\n",
    "        train_set.append(test_set.pop(index))\n",
    "    return train_set, test_set\n",
    "\n",
    "# Function to calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Function to evaluate an algorithm using a train/test split\n",
    "def evaluate_algorithm(dataset, algorithm, split_ratio, *args):\n",
    "    train_set, test_set = train_test_split(dataset, split_ratio)\n",
    "    test_set_copy = list(test_set)\n",
    "    predicted = algorithm(train_set, test_set, *args)\n",
    "    actual = [row[-1] for row in test_set_copy]\n",
    "    accuracy = accuracy_metric(actual, predicted)\n",
    "    return accuracy\n",
    "\n",
    "# Function to make predictions with a Random Forest classifier\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees):\n",
    "    trees = []\n",
    "    for _ in range(n_trees):\n",
    "        sample = random.sample(train, k=sample_size)\n",
    "        tree = build_tree(sample, max_depth, min_size)\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return predictions\n",
    "\n",
    "# Function to build a decision tree\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    root = get_split(train)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root\n",
    "\n",
    "# Function to split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = [], []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Function to calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p * p\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini\n",
    "\n",
    "# Function to select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
    "\n",
    "# Function to create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Function to create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)\n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split(node['right'], max_depth, min_size, depth+1)\n",
    "\n",
    "# Function to make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(predictions), key=predictions.count)\n",
    "\n",
    "# Function to classify a given row using a decision tree\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        value = row[column]\n",
    "        # Check if the value is numeric\n",
    "        if isinstance(value, str) and value.replace('.', '', 1).isdigit():  # Check if string is numeric\n",
    "            row[column] = float(value.strip())\n",
    "        # Handle non-numeric data or leave it as is\n",
    "        # You can add additional checks or handling as needed\n",
    "\n",
    "# Function to convert string column to integer (if applicable)\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = {value: index for index, value in enumerate(unique)}\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees):\n",
    "    trees = []\n",
    "    for _ in range(n_trees):\n",
    "        sample = random.sample(train, k=int(sample_size))  # Convert sample_size to int\n",
    "        tree = build_tree(sample, max_depth, min_size)\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return predictions\n",
    "\n",
    "# Main code to run the random forest classifier without using external libraries\n",
    "if __name__ == '__main__':\n",
    "    # Load dataset\n",
    "    filename = 'Dataset/training_data.csv'\n",
    "    dataset = load_csv(filename)\n",
    "\n",
    "    # Convert string attributes to appropriate types\n",
    "    for i in range(len(dataset[0])):\n",
    "        str_column_to_float(dataset, i)\n",
    "\n",
    "    # Convert class column to integers\n",
    "    str_column_to_int(dataset, len(dataset[0])-1)  # Assuming last column is the target variable\n",
    "\n",
    "    # Evaluate algorithm\n",
    "    split_ratio = 0.8\n",
    "    max_depth = 10\n",
    "    min_size = 1\n",
    "    sample_size = 1.0\n",
    "    n_trees = 10\n",
    "    accuracy = evaluate_algorithm(dataset, random_forest, split_ratio, max_depth, min_size, sample_size, n_trees)\n",
    "    print('Accuracy: %.2f%%' % accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
